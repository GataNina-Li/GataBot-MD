import fetch from 'node-fetch'
import fs from 'fs'
import path from 'path'
import axios from 'axios'
import translate from '@vitalets/google-translate-api'
import { perplexity } from '../lib/chatgpt.js'
import { Configuration, OpenAIApi } from 'openai'

const apikey_base64 =
'c2stcHJvai1tUzN4bGZueXo0UjBPWV8zbm1DVDlMQmlmYXhYbVdaa0ptUVFJMDVKR2FxdHZCbk9ncWZjRXdCbEJmMU5WN0lYa0pncVJuM3BNc1QzQmxia0ZKMVJ5aEJzUl93NzRXbll5LWdjdkowT0NQUXliWTBOcENCcDZIOTlCVVVtcWxuTjVraEZxMk43TGlMU0RsU0s1cXA5Tm1kWVZXc0E='

const apikey = Buffer.from(apikey_base64, 'base64').toString('utf-8')
const configuration = new Configuration({apiKey: apikey})
const openai = new OpenAIApi(configuration)

const handler = async (m, {conn, text, usedPrefix, command}) => {
if (usedPrefix == 'a' || usedPrefix == 'A') return
if (!text)
throw `*${lenguajeGB['smsAvisoMG']()}ùôÑùôâùôÇùôçùôÄùôéùôÄ ùôêùôâùòº ùôãùôÄùôèùôÑùòæùôÑùôäùôâ ùôä ùôêùôâùòº ùôäùôçùòøùôÄùôâ ùôãùòºùôçùòº ùôêùôéùòºùôç ùôáùòº ùôÅùôêùôâùòæùôÑùôäùôâ ùòøùôÄùôá ùòæùôÉùòºùôèùôÇùôãùôè\n\n‚ùè ùôÄùôÖùôÄùôàùôãùôáùôä ùòøùôÄ ùôãùôÄùôèùôÑùòæùôÑùôäùôâùôÄùôé ùôî ùôäùôçùòøùôÄùôâùôÄùôé\n‚ùè ${usedPrefix + command} Recomienda un top 10 de pel√≠culas de acci√≥n\n‚ùè ${usedPrefix + command} Codigo en JS para un juego de cartas`
//let syms1 = `Actuaras como un Bot de WhatsApp el cual fue creado por GataNina-Li, tu seras GataBot-MD üêà`;
let syms1 = await fetch('https://raw.githubusercontent.com/Skidy89/chat-gpt-jailbreak/main/Text.txt').then((v) => v.text())

if (command == 'ia' || command == 'chatgpt') {
try {
const messages = [
{role: 'system', content: syms1},
{role: 'user', content: text}
]

const chooseModel = (query) => {
const lowerText = query.toLowerCase()

if (lowerText.includes('c√≥digo') || lowerText.includes('programaci√≥n') || lowerText.includes('code') || lowerText.includes('script')) {
return 'codellama-70b-instruct'
} else if (lowerText.includes('noticias') || lowerText.includes('actual') || lowerText.includes('hoy') || lowerText.includes('√∫ltimo')) {
return 'sonar-medium-online'
} else if (lowerText.includes('explica') || lowerText.includes('por qu√©') || lowerText.includes('razona') || lowerText.includes('analiza')) {
return 'sonar-reasoning-pro'
} else if (lowerText.includes('c√≥mo') || lowerText.includes('paso a paso') || lowerText.includes('instrucciones')) {
return 'mixtral-8x7b-instruct'
} else if (lowerText.includes('charla') || lowerText.includes('habla') || lowerText.includes('dime')) {
return 'sonar-medium-chat'
} else {
return 'sonar-pro'
}
}

const selectedModel = chooseModel(text)
const fallbackModels = Object.keys(perplexity.api.models).filter((m) => m !== selectedModel)
let response = await perplexity.chat(messages, selectedModel)

if (!response.status) {
for (const fallback of fallbackModels) {
try {
response = await perplexity.chat(messages, fallback)
if (response.status) {
//console.log(`Respaldo ${fallback} funcion√≥`);
break
}
} catch (e) {
console.error(`Fall√≥ ${fallback}: ${e.message}`)
}
}
}

if (response.status) {
await m.reply(response.result.response)
}
} catch {
try {
async function getResponse(prompt) {
try {
await delay(1000)
const response = await axios.post(
'https://api.openai.com/v1/chat/completions',
{model: 'gpt-4o-mini', messages: [{role: 'user', content: prompt}], max_tokens: 300},
{
headers: {
'Content-Type': 'application/json',
Authorization: `Bearer ${apikey}`
}
}
)
return response.data.choices[0].message.content
} catch (error) {
console.error(error)
}
}

const respuesta = await getResponse(text)
m.reply(respuesta)
} catch {
try {
let gpt = await fetch(`${apis}/ia/gptweb?text=${text}`)
let res = await gpt.json()
await m.reply(res.gpt)
/*let gpt = await fetch(`https://deliriusapi-official.vercel.app/ia/chatgpt?q=${text}`)
let res = await gpt.json()
await m.reply(res.data)*/
} catch {}
}
}
}

if (command == 'openai' || command == 'ia2' || command == 'chatgpt2') {
conn.sendPresenceUpdate('composing', m.chat)
let gpt = await fetch(`${apis}/ia/gptweb?text=${text}`)
let res = await gpt.json()
await m.reply(res.gpt)
}
}
handler.command = /^(openai|chatgpt|ia|ai|openai2|chatgpt2|ia2)$/i
export default handler
